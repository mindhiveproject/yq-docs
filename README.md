---
description: What is the quantified self and what's the purpose of this app?
---

# ðŸ‘‹ Introduction to You: Quantified

## Overview

### Platform

This platform provides a medium for artists, scientists, educators, and students to explore data generated by their brains and bodies. It does so by connecting a variety of bio-sensing devices to a creative coding sandbox.

{% hint style="info" %}
Currently, the app has restricted access. You can join the waitlist at [youquantified.app](https://youquantified.app/) or reach out directly to us via email at youquantified@nyu.edu
{% endhint %}

### Curriculum

Alongside the platform, we are developing an educational curriculum that uses this application as a learning tool and covers topics such as data, bio-sensing, social synchrony, and artificial intelligence.

* You can find more information in the [educational resources](educational-resources.md) section of this guide.

## Platform usage

If this is your first time visiting the platform, please check out our [quick start guide](quick-start.md).&#x20;

{% content-ref url="quick-start.md" %}
[quick-start.md](quick-start.md)
{% endcontent-ref %}

### Summary of devices

Below, we provide a summary of the devices that may be used with the web application. Some of them may require the purchase of separate hardware while others may be used with your computer's webcam or microphone. You can find more information under the [supported devices](broken-reference) tab.

<table data-view="cards" data-full-width="false"><thead><tr><th>Device</th><th>What does it stream?</th><th>Pre-requirements</th><th data-type="content-ref"></th><th data-hidden data-card-target data-type="content-ref"></th></tr></thead><tbody><tr><td><strong>File upload</strong></td><td>Pre-recorded data</td><td>A previous recording with the app.</td><td></td><td><a href="devices/file-upload.md">file-upload.md</a></td></tr><tr><td><strong>Face Landmarks</strong></td><td>Facial expression metrics</td><td>Camera</td><td></td><td><a href="devices/face-landmarks.md">face-landmarks.md</a></td></tr><tr><td><strong>Video Heart Rate</strong></td><td>Heart rate</td><td>Camera</td><td></td><td><a href="devices/video-heart-rate.md">video-heart-rate.md</a></td></tr><tr><td><strong>Audio Volume</strong></td><td>Ambient noise levels</td><td>Microphone</td><td></td><td></td></tr><tr><td><strong>Muse</strong></td><td><p>EEG &#x26; bandpower</p><p>PPG &#x26; heart rate</p></td><td>Muse 2 hardware</td><td></td><td><a href="devices/muse.md">muse.md</a></td></tr><tr><td><strong>EMOTIV</strong></td><td>EEG &#x26; band power</td><td>EMOTIV-hardware</td><td></td><td><a href="devices/emotiv.md">emotiv.md</a></td></tr><tr><td><strong>LSL</strong></td><td>Multiple types of streams</td><td>LSL-Compatible Scientific Equipment</td><td></td><td></td></tr></tbody></table>

## Contributing to development

This app was built as an open-source platform. For project maintenance and documentation, you may find some information in the developer's section of this guide and check out the full code powering the web application at the following GitHub.

{% embed url="https://github.com/esromerog/You-Quantified/tree/main" %}

As an open-source platform, this app leverages other technologies, such as [P5.js](https://p5js.org). You may find more information as you dive into this documentation.

### About us & Credits

This app was developed at NYU through a collaboration of multiple research laboratories as a project aimed at promoting studentsâ€™ data literacy through their creative representation of the Quantified Self.  &#x20;

Our team is also responsible for the development, maintenance, and implementation of the [MindHive platform](https://mindhive.science).

Some of the lead researchers include Dr [Suzanne Dikker](http://www.suzannedikker.net), Dr [Camilia Matuk](https://wp.nyu.edu/riddle/), and Dr [Xavier Ochoa](https://steinhardt.nyu.edu/people/xavier-ochoa).



&#x20;



